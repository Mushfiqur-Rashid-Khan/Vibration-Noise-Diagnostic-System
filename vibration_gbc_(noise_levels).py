# -*- coding: utf-8 -*-
"""Vibration_GBC (Noise Levels).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zbs97-M2y_XOUHARO7spq2Calvz-VfRr
"""

pip install catboost optuna ngboost

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, confusion_matrix
)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
from sklearn.multiclass import OneVsRestClassifier
from ngboost import NGBClassifier
import tensorflow as tf
import optuna
import warnings
warnings.filterwarnings("ignore")

# Load dataset
df = pd.read_csv("/content/Vibration_Dataset.csv")

# Combine binary fault columns into multi-class label
fault_labels = [
    'Signal Drift', 'Gas Leakage', 'High Noise Levels',
    'Radiation-Induced Degradation', 'Thermal Drift',
    'Sensor Aging (Degradation)', 'Offset Bias', 'Wiring Reversal'
]
df["Target"] = df[fault_labels].apply(lambda row: np.argmax(row.values) if row.sum() > 0 else -1, axis=1)
df = df[df["Target"] != -1]

# Features and labels
X = df.drop(columns=fault_labels)
y = df["Signal Drift"]

# Split data (70:30)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# Standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SMOTE
sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X_train_scaled, y_train)

# PCA
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_res)
X_test_pca = pca.transform(X_test_scaled)

# Storage for results
model_scores = {}

# ---- MODEL 1: SGBoost ----
def objective_sgboost(trial):
    clf = GradientBoostingClassifier(
        n_estimators=trial.suggest_int("n_estimators", 50, 200),
        learning_rate=trial.suggest_float("learning_rate", 0.01, 0.3),
        max_depth=trial.suggest_int("max_depth", 2, 8)
    )
    clf.fit(X_train_pca, y_res)
    preds = clf.predict(X_test_pca)
    return accuracy_score(y_test, preds)

study = optuna.create_study(direction="maximize")
study.optimize(objective_sgboost, n_trials=20)
sg_model = GradientBoostingClassifier(**study.best_params)
sg_model.fit(X_train_pca, y_res)
model_scores["SGBoost"] = accuracy_score(y_test, sg_model.predict(X_test_pca))

# ---- MODEL 2: XGBoost ----
def objective_xgb(trial):
    clf = xgb.XGBClassifier(
        n_estimators=trial.suggest_int("n_estimators", 50, 200),
        learning_rate=trial.suggest_float("learning_rate", 0.01, 0.3),
        max_depth=trial.suggest_int("max_depth", 3, 10),
        use_label_encoder=False,
        eval_metric='mlogloss'
    )
    clf.fit(X_train_pca, y_res)
    preds = clf.predict(X_test_pca)
    return accuracy_score(y_test, preds)

study = optuna.create_study(direction="maximize")
study.optimize(objective_xgb, n_trials=20)
xgb_model = xgb.XGBClassifier(**study.best_params, use_label_encoder=False, eval_metric="mlogloss")
xgb_model.fit(X_train_pca, y_res)
model_scores["XGBoost"] = accuracy_score(y_test, xgb_model.predict(X_test_pca))

# ---- MODEL 3: Cost-sensitive LightGBM ----
def objective_lgb(trial):
    clf = lgb.LGBMClassifier(
        n_estimators=trial.suggest_int("n_estimators", 50, 200),
        learning_rate=trial.suggest_float("learning_rate", 0.01, 0.3),
        max_depth=trial.suggest_int("max_depth", 3, 10),
        class_weight="balanced"
    )
    clf.fit(X_train_pca, y_res)
    preds = clf.predict(X_test_pca)
    return accuracy_score(y_test, preds)

study = optuna.create_study(direction="maximize")
study.optimize(objective_lgb, n_trials=20)
lgb_model = lgb.LGBMClassifier(**study.best_params, class_weight="balanced")
lgb_model.fit(X_train_pca, y_res)
model_scores["LightGBM"] = accuracy_score(y_test, lgb_model.predict(X_test_pca))

# ---- MODEL 4: CatBoost ----
cat_model = CatBoostClassifier(verbose=0, auto_class_weights='Balanced')
cat_model.fit(X_train_pca, y_res)
model_scores["CatBoost"] = accuracy_score(y_test, cat_model.predict(X_test_pca))

# ---- MODEL 5: NGBoost ----
ng_model = OneVsRestClassifier(NGBClassifier(verbose=False))
ng_model.fit(X_train_pca, y_res)
model_scores["NGBoost"] = accuracy_score(y_test, ng_model.predict(X_test_pca))
"""
# ---- MODEL 6: TensorFlow Boosted Trees ----
import tensorflow_decision_forests as tfdf
df_train_tf = pd.DataFrame(X_train_pca)
df_train_tf["label"] = y_res.values
df_test_tf = pd.DataFrame(X_test_pca)
df_test_tf["label"] = y_test.values

model_tf = tfdf.keras.GradientBoostedTreesModel(task=tfdf.keras.Task.CLASSIFICATION)
model_tf.fit(df_train_tf)
eval_tf = model_tf.evaluate(df_test_tf, return_dict=True)
model_scores["TFBoostedTrees"] = eval_tf["accuracy"]
"""
# ---- Final Results ----
print("Final Model Accuracies:")
for model, score in model_scores.items():
    print(f"{model}: {score:.4f}")

best_model = max(model_scores, key=model_scores.get)
print(f"\n✅ Best Model: {best_model} with Accuracy: {model_scores[best_model]:.4f}")

"""**SGBoost**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.decomposition import PCA
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset
df = pd.read_csv('/content/Vibration_Dataset.csv')

# 2. Feature & target separation
X = df.iloc[:, :-3]
y = df.iloc[:, -3]

# 3. Train-test split (70:30)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# 4. Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Apply SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train_scaled, y_train)

# 6. PCA
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_res)
X_test_pca = pca.transform(X_test_scaled)

# 7. Train SGBoost
sgboost_model = GradientBoostingClassifier(random_state=42)
sgboost_model.fit(X_train_pca, y_res)

# 8. Predict
y_pred = sgboost_model.predict(X_test_pca)

# 9. Evaluation
print("✅ SGBoost Evaluation Results:")
print("Accuracy:", accuracy_score(y_test, y_pred)*100,'%')
print("Classification Report:\n", classification_report(y_test, y_pred))

# 10. Colorful Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
            xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])
plt.title('SGBoost (Noise Levels)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Specificity = TN / (TN + FP)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)

print(f"Precision: {precision:.2f}")
print(f"Recall (Sensitivity): {recall:.2f}")
print(f"Specificity: {specificity:.2f}")

"""**XGBoost**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.decomposition import PCA
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset
df = pd.read_csv('/content/Vibration_Dataset.csv')

# 2. Feature & target separation
X = df.iloc[:, :-3]
y = df.iloc[:, -3]

# 3. Train-test split (70:30)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# 4. Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Apply SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train_scaled, y_train)

# 6. PCA
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_res)
X_test_pca = pca.transform(X_test_scaled)

# 7. Train XGBoost
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xgb_model.fit(X_train_pca, y_res)

# 8. Predict
y_pred = xgb_model.predict(X_test_pca)

# 9. Evaluation
print("XGBoost Evaluation Results:")
print("Accuracy:", accuracy_score(y_test, y_pred)*100, '%')
print("Classification Report:\n", classification_report(y_test, y_pred))

# 10. Colorful Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=True,
            xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])
plt.title('XGBoost (Noise Levels)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# Metrics
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Specificity = TN / (TN + FP)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)


print(f"Precision: {precision:.2f}")
print(f"Recall (Sensitivity): {recall:.2f}")
print(f"Specificity: {specificity:.2f}")

"""**Cost Sensitive LightGBM**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.decomposition import PCA
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# 1. Load the dataset
df = pd.read_csv('/content/Vibration_Dataset.csv')

# 2. Feature & target separation
X = df.iloc[:, :-3]
y = df.iloc[:, -3]

# 3. Train-test split (70:30)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# 4. Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Apply SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train_scaled, y_train)

# 6. PCA
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_res)
X_test_pca = pca.transform(X_test_scaled)

# 7. Calculate class weights for cost-sensitive learning
class_counts = Counter(y_res)
total = sum(class_counts.values())
class_weights = {cls: total / (len(class_counts) * count) for cls, count in class_counts.items()}
sample_weights = np.array([class_weights[label] for label in y_res])

# 8. Train Cost-Sensitive LightGBM
lgb_model = LGBMClassifier(random_state=42)
lgb_model.fit(X_train_pca, y_res, sample_weight=sample_weights)

# 9. Predict
y_pred = lgb_model.predict(X_test_pca)

# 10. Evaluation
print("Cost-Sensitive LightGBM Evaluation Results:")
print("Accuracy:", accuracy_score(y_test, y_pred)*100,'%')
print("Classification Report:\n", classification_report(y_test, y_pred))

# 11. Colorful Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', cbar=True,
            xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])
plt.title('Cost-Sensitive LightGBM (Noise Levels)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Specificity = TN / (TN + FP)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)

#print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall (Sensitivity): {recall:.2f}")
print(f"Specificity: {specificity:.2f}")

"""**Hybrid LightGBM**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import VotingClassifier
import lightgbm as lgb
from sklearn.preprocessing import StandardScaler

# 1. Load the dataset (adjust the file path as necessary)
df = pd.read_csv('/content/Vibration_Dataset.csv')

# 2. Feature & target separation
X = df.iloc[:, :-3]
y = df.iloc[:, -3]

# Check for missing values and handle them (if any)
X = X.fillna(X.mean())  # Impute missing values with column mean

# 3. Feature Scaling (especially important for Naive Bayes)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset into training and testing sets (30% testing)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=None)

# 4. LightGBM with hyperparameter tuning using GridSearchCV
lgbm = lgb.LGBMClassifier()

lgbm_param_grid = {
    'num_leaves': [60, 100],
    'learning_rate': [0.01, 0.05],
    'n_estimators': [100, 200]
}

lgbm_grid_search = GridSearchCV(lgbm, lgbm_param_grid, cv=10, n_jobs=-1, verbose=1)
lgbm_grid_search.fit(X_train, y_train)

best_lgbm = lgbm_grid_search.best_estimator_
print("Best parameters for LightGBM:", lgbm_grid_search.best_params_)

# 5. Gaussian Naive Bayes (no hyperparameter tuning for simplicity)
gnb = GaussianNB()

# 6. Create a voting ensemble classifier with LightGBM and Gaussian Naive Bayes
voting_model = VotingClassifier(
    estimators=[
        ('lgbm', best_lgbm),
        ('gnb', gnb)
    ], voting='hard'  # Soft voting uses predicted probabilities
)

# 7. Cross-validation (to evaluate model performance)
cross_val_score_result = cross_val_score(voting_model, X_train, y_train, cv=10, scoring='accuracy')
print(f'Cross-validation accuracy scores: {cross_val_score_result}')
print(f'Mean accuracy from cross-validation: {cross_val_score_result.mean()}')

# 8. Train the hybrid model on the entire training set
voting_model.fit(X_train, y_train)

# 9. Predict on the test set
y_pred = voting_model.predict(X_test)

# 10. Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

# Detailed classification report
print(classification_report(y_test, y_pred))

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print(f'Confusion Matrix:\n{conf_matrix}')

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', cbar=True,
            xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])
plt.title('Hybrid LightGBM (Noise Levels)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Specificity = TN / (TN + FP)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)

print(f"Accuracy: {accuracy:.6f}")
print(f"Precision: {precision:.2f}")
print(f"Recall (Sensitivity): {recall:.2f}")
print(f"Specificity: {specificity:.2f}")

"""**CatBoost**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.decomposition import PCA
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset
df = pd.read_csv('/content/Vibration_Dataset.csv')

# 2. Feature & target separation
X = df.iloc[:, :-3]
y = df.iloc[:, -3]

# 3. Train-test split (70:30)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# 4. Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Apply SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train_scaled, y_train)

# 6. PCA
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_res)
X_test_pca = pca.transform(X_test_scaled)

# 7. Train CatBoost (silent mode for clean output)
catboost_model = CatBoostClassifier(verbose=0, random_state=42)
catboost_model.fit(X_train_pca, y_res)

# 8. Predict
y_pred = catboost_model.predict(X_test_pca)

# 9. Evaluation
print("✅ CatBoost Evaluation Results:")
print("Accuracy:", accuracy_score(y_test, y_pred)*100,'%')
print("Classification Report:\n", classification_report(y_test, y_pred))

# 10. Colorful Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', cbar=True,
            xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])
plt.title('CatBoost (Noise Levels)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Specificity = TN / (TN + FP)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall (Sensitivity): {recall:.2f}")
print(f"Specificity: {specificity:.2f}")

"""**Natural Gradient Boosting (NGBoost)**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.decomposition import PCA
from sklearn.multiclass import OneVsRestClassifier
from ngboost import NGBClassifier
from ngboost.scores import LogScore
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset
df = pd.read_csv('/content/Vibration_Dataset.csv')

# 2. Feature & target separation
X = df.iloc[:, :-4]
y = df.iloc[:, -3]

# 3. Train-test split (70:30)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# 4. Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Apply SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train_scaled, y_train)

# 6. PCA
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_res)
X_test_pca = pca.transform(X_test_scaled)

# 7. Train NGBoost with OneVsRestClassifier distribution
ng_model = OneVsRestClassifier(NGBClassifier(verbose=False))
ng_model.fit(X_train_pca, y_res)
model_scores["NGBoost"] = accuracy_score(y_test, ng_model.predict(X_test_pca))

# 8. Predict
y_pred = ng_model.predict(X_test_pca)

# 9. Evaluation
print("✅ NGBoost Evaluation Results:")
print("Accuracy:", accuracy_score(y_test, y_pred)*100,'%')
print("Classification Report:\n", classification_report(y_test, y_pred))

# 10. Colorful Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
            xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])
plt.title('NGBoost (Noise Levels)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Specificity = TN / (TN + FP)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall (Sensitivity): {recall:.2f}")
print(f"Specificity: {specificity:.2f}")

"""**Tensor Flow Boosted Trees**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow_decision_forests as tfdf
import tensorflow as tf

# 1. Load dataset
df = pd.read_csv('/content/Vibration_Dataset.csv')

# 2. Feature and target separation
X = df.iloc[:, :-3]
y = df.iloc[:, -3].astype(str)  # Convert labels to strings for TFDF

# 3. Train-test split (70:30)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# 4. Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train_scaled, y_train)

# 6. PCA
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_res)
X_test_pca = pca.transform(X_test_scaled)

# 7. Prepare for TFDF (TFDF expects DataFrame with string labels)
X_train_df = pd.DataFrame(X_train_pca, columns=[f'PC{i}' for i in range(X_train_pca.shape[1])])
X_train_df['label'] = y_res

X_test_df = pd.DataFrame(X_test_pca, columns=[f'PC{i}' for i in range(X_test_pca.shape[1])])
X_test_df['label'] = y_test.values

# 8. Convert to TF datasets
train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(X_train_df, label="label")
test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(X_test_df, label="label")

# 9. Train TFTB model
model = tfdf.keras.GradientBoostedTreesModel()
model.fit(train_ds)

# 10. Predict
y_pred_probs = model.predict(test_ds)
y_pred_classes = np.argmax(y_pred_probs, axis=1)
class_labels = sorted(y.unique())
y_pred = [class_labels[i] for i in y_pred_classes]

# 11. Evaluation
print("✅ TensorFlow Boosted Trees Evaluation Results:")
print("Accuracy:", accuracy_score(y_test, y_pred)*100,'%')
print("Classification Report:\n", classification_report(y_test, y_pred))

# 12. Colorful Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])
plt.title('TensorFlow Gradient Boosted Trees (Noise Levels)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

"""**GBDT++**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE

# 1. Load dataset
df = pd.read_csv('/content/Vibration_Dataset.csv')

# 2. Split features and labels
X = df.iloc[:, :-2]
y = df.iloc[:, -2]

# 3. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# 4. Standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. SMOTE oversampling
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train_scaled, y_train)

# 6. PCA
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_res)
X_test_pca = pca.transform(X_test_scaled)

# 7. Train GBDT model
gbdt = GradientBoostingClassifier(random_state=42)
gbdt.fit(X_train_pca, y_res)

# 8. Predict and evaluate
y_pred = gbdt.predict(X_test_pca)

print("✅ GBDT Evaluation Results:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# 9. Confusion matrix
cm = confusion_matrix(y_test, y_pred)
class_labels = sorted(y.unique())
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='PuBu', xticklabels=class_labels, yticklabels=class_labels)
plt.title("🎯 GBDT Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Specificity = TN / (TN + FP)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall (Sensitivity): {recall:.2f}")
print(f"Specificity: {specificity:.2f}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
from scipy.stats import uniform, randint

# Load dataset
df = pd.read_csv('/content/Vibration_Dataset.csv')

# Features and label
X = df.iloc[:, :-3]
y = df.iloc[:, -3]

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# Standardize
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train_scaled, y_train)

# PCA
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_res)
X_test_pca = pca.transform(X_test_scaled)

# Hyperparameter tuning for GBDT
param_dist = {
    'n_estimators': randint(100, 500),
    'learning_rate': uniform(0.01, 0.2),
    'max_depth': randint(3, 10),
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 10),
    'subsample': uniform(0.6, 0.4)
}

gbdt = GradientBoostingClassifier(random_state=42)

random_search = RandomizedSearchCV(gbdt, param_distributions=param_dist,
                                   n_iter=30, cv=3, scoring='accuracy',
                                   random_state=42, n_jobs=-1, verbose=1)

random_search.fit(X_train_pca, y_res)
best_gbdt = random_search.best_estimator_

# Evaluation
y_pred = best_gbdt.predict(X_test_pca)
print("🎯 Improved GBDT Evaluation:")
print("Best Parameters:", random_search.best_params_)
print("Accuracy:", accuracy_score(y_test, y_pred)*100, '%')
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
class_labels = sorted(y.unique())
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])
plt.title("GBDT++ (Noise Levels)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Specificity = TN / (TN + FP)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall (Sensitivity): {recall:.2f}")
print(f"Specificity: {specificity:.2f}")